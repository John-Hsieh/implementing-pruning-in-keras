{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class pruned_Dense(Layer):\n",
    "    def __init__(self, n_neurons_out, **kwargs):\n",
    "        self.n_neurons_out = n_neurons_out\n",
    "        super(pruned_Dense,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #define the variables of this layer in the build function:\n",
    "        n_neurons_in = input_shape[1]\n",
    "        # print(n_neurons_in)\n",
    "        # print(self.n_neurons_out)\n",
    "        stdv = 1/np.sqrt(n_neurons_in)\n",
    "        w = np.random.normal(size=[n_neurons_in, self.n_neurons_out], loc=0.0, scale=stdv).astype(np.float32)\n",
    "        self.w = K.variable(w)\n",
    "        b = np.zeros(self.n_neurons_out)\n",
    "        self.b = K.variable(b)\n",
    "        # w is the weight matrix, b is the bias. These are the trainable variables of this layer.\n",
    "        self.trainable_weights = [self.w, self.b]\n",
    "        # mask is a non-trainable weight that simulates pruning. the values of mask should be either 1 or 0, where 0 will prune a weight. We initialize mask to all ones:\n",
    "        mask = np.ones((n_neurons_in, self.n_neurons_out))\n",
    "        self.mask = K.variable(mask)\n",
    "\n",
    "    def call(self, x):\n",
    "        # define the input-output relationship in this layer in this function\n",
    "        pruned_w = self.w * self.mask\n",
    "        out = K.dot(x, pruned_w)\n",
    "        out = out + self.b\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #define the shape of this layer's output:\n",
    "        return (input_shape[0], self.n_neurons_out)\n",
    "\n",
    "    def get_mask(self):\n",
    "        #get the mask values\n",
    "        return K.get_value(self.mask)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        #set new mask values to this layer\n",
    "        K.set_value(self.mask, mask)\n",
    "\n",
    "class pruned_Conv2D(Layer):\n",
    "    def __init__(self,n_neurons_out):\n",
    "        return\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "    def call(self, x):\n",
    "        return\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return\n",
    "    def get_mask(self):\n",
    "        return\n",
    "    def set_mask(self, mask):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Tensor(\"conv2d_97/BiasAdd:0\", shape=(?, 30, 30, 64), dtype=float32)\n",
      "Tensor(\"activation_141/Relu:0\", shape=(?, 30, 30, 64), dtype=float32)\n",
      "Tensor(\"batch_normalization_127/cond/Merge:0\", shape=(?, 30, 30, 64), dtype=float32)\n",
      "Tensor(\"conv2d_98/BiasAdd:0\", shape=(?, 28, 28, 64), dtype=float32)\n",
      "Tensor(\"activation_142/Relu:0\", shape=(?, 28, 28, 64), dtype=float32)\n",
      "Tensor(\"batch_normalization_128/cond/Merge:0\", shape=(?, 28, 28, 64), dtype=float32)\n",
      "Tensor(\"max_pooling2d_34/MaxPool:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"conv2d_99/BiasAdd:0\", shape=(?, 12, 12, 128), dtype=float32)\n",
      "Tensor(\"activation_143/Relu:0\", shape=(?, 12, 12, 128), dtype=float32)\n",
      "Tensor(\"batch_normalization_129/cond/Merge:0\", shape=(?, 12, 12, 128), dtype=float32)\n",
      "Tensor(\"conv2d_100/BiasAdd:0\", shape=(?, 10, 10, 128), dtype=float32)\n",
      "Tensor(\"activation_144/Relu:0\", shape=(?, 10, 10, 128), dtype=float32)\n",
      "Tensor(\"batch_normalization_130/cond/Merge:0\", shape=(?, 10, 10, 128), dtype=float32)\n",
      "Tensor(\"max_pooling2d_35/MaxPool:0\", shape=(?, 5, 5, 128), dtype=float32)\n",
      "Tensor(\"conv2d_101/BiasAdd:0\", shape=(?, 3, 3, 256), dtype=float32)\n",
      "Tensor(\"activation_145/Relu:0\", shape=(?, 3, 3, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_131/cond/Merge:0\", shape=(?, 3, 3, 256), dtype=float32)\n",
      "Tensor(\"conv2d_102/BiasAdd:0\", shape=(?, 1, 1, 256), dtype=float32)\n",
      "Tensor(\"activation_146/Relu:0\", shape=(?, 1, 1, 256), dtype=float32)\n",
      "Tensor(\"batch_normalization_132/cond/Merge:0\", shape=(?, 1, 1, 256), dtype=float32)\n",
      "Tensor(\"flatten_16/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dense_46/BiasAdd:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"activation_147/Relu:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_133/cond/Merge:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"dense_47/BiasAdd:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"activation_148/Relu:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"batch_normalization_134/cond/Merge:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"dense_48/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"activation_149/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "10000/10000 [==============================] - 44s 4ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "data_train = X_train\n",
    "labels_train = Y_train\n",
    "data_test = X_test\n",
    "labels_test = Y_test\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size=100\n",
    "lr=0.001\n",
    "Training=True\n",
    "Compressing=False\n",
    "\n",
    "def get_model():\n",
    "\tbatch_norm_alpha=0.9\n",
    "\tbatch_norm_eps=1e-4\n",
    "\n",
    "\tmodel=Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\t#model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\n",
    "\tmodel.add(Dense(512))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Dense(512))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Dense(10))\n",
    "\tmodel.add(Activation('softmax'))\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "model=get_model()\n",
    "for i in range(len(model.layers)):\n",
    " \n",
    "    print(model.get_layer(index=i).output)\n",
    "\n",
    "weights_path='pretrained_cifar10.h5'\n",
    "model.load_weights(weights_path)\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#loss1 = tf.keras.losses.sparse_categorical_crossentropy()\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "scores_dropout_hidden = model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))\n",
    "#complie the model with sparse categorical crossentropy loss function as you did in part 1\n",
    "\n",
    "#make sure weights are loaded correctly by evaluating the model here and printing the output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_inbound_node', '_base_init', '_build_input_shape', '_built', '_check_trainable_weights_consistency', '_collected_trainable_weights', '_compute_previous_mask', '_expects_training_arg', '_feed_input_names', '_feed_input_shapes', '_feed_inputs', '_feed_loss_fns', '_feed_output_names', '_feed_output_shapes', '_feed_outputs', '_feed_sample_weight_modes', '_feed_sample_weights', '_feed_targets', '_function_kwargs', '_get_node_attribute_at_index', '_inbound_nodes', '_init_graph_network', '_init_subclassed_network', '_initial_weights', '_input_coordinates', '_input_layers', '_is_compiled', '_is_graph_network', '_layers', '_layers_by_depth', '_losses', '_make_predict_function', '_make_test_function', '_make_train_function', '_network_nodes', '_node_key', '_nodes_by_depth', '_outbound_nodes', '_output_coordinates', '_output_layers', '_output_mask_cache', '_output_shape_cache', '_output_tensor_cache', '_per_input_losses', '_per_input_updates', '_set_inputs', '_standardize_user_data', '_updated_config', '_updates', '_uses_dynamic_learning_phase', '_uses_inputs_arg', 'add', 'add_loss', 'add_update', 'add_weight', 'assert_input_compatibility', 'build', 'built', 'call', 'compile', 'compute_mask', 'compute_output_shape', 'count_params', 'evaluate', 'evaluate_generator', 'fit', 'fit_generator', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', 'get_losses_for', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_updates_for', 'get_weights', 'input', 'input_mask', 'input_names', 'input_shape', 'input_spec', 'inputs', 'layers', 'load_weights', 'loss', 'loss_functions', 'loss_weights', 'losses', 'metrics', 'metrics_names', 'metrics_tensors', 'metrics_updates', 'model', 'name', 'non_trainable_weights', 'optimizer', 'output', 'output_mask', 'output_names', 'output_shape', 'outputs', 'pop', 'predict', 'predict_classes', 'predict_function', 'predict_generator', 'predict_on_batch', 'predict_proba', 'reset_states', 'run_internal_graph', 'sample_weight_mode', 'sample_weight_modes', 'sample_weights', 'save', 'save_weights', 'set_weights', 'state_updates', 'stateful', 'stateful_metric_functions', 'stateful_metric_names', 'summary', 'supports_masking', 'targets', 'test_function', 'test_on_batch', 'to_json', 'to_yaml', 'total_loss', 'train_function', 'train_on_batch', 'trainable', 'trainable_weights', 'updates', 'uses_learning_phase', 'weighted_metrics', 'weights']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "ooooh\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Flatten'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n"
     ]
    }
   ],
   "source": [
    "for i in model.layers:\n",
    "    print(type(i))\n",
    "    if type(i) == keras.layers.convolutional.Conv2D:\n",
    "        print(\"ooooh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__func__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model.add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
      "<class 'utils.pruned_Dense'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'utils.pruned_Dense'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'utils.pruned_Dense'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Flatten'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "#Discarded\n",
    "\n",
    "def convert_to_masked_model(model):\n",
    "    #implement a function that takes a model and returns another model with masked_conv and masked_dense layers\n",
    "    index = 0;\n",
    "    print(dir(model.layers))\n",
    "    \n",
    "    for i in model.layers:\n",
    "        if type(i) == keras.layers.core.Dense:\n",
    "            a = i.input_shape;\n",
    "            temp = pruned_Dense(i.output_shape[1])\n",
    "            temp.build(a)\n",
    "            temp.set_mask(temp.get_mask())\n",
    "            print(type(temp))\n",
    "            model.layers[index] = temp;\n",
    "            print(type(model.layers[index]))\n",
    "        index = index+1;\n",
    "    return model\n",
    "#convert the layers to maskable_layers:\n",
    "prunable_model=convert_to_masked_model(model)\n",
    "for i in prunable_model.layers:\n",
    "    print(type(i))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "prunable_model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "#scores_dropout_hidden = prunable_model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))\n",
    "\n",
    "#do the rest of the project:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if type(i) == keras.layers.core.Dense:\n",
    "            \n",
    "            print(dir(i))\n",
    "            a = i.input_shape;\n",
    "            temp = pruned_Dense(i.output_shape[1])\n",
    "            \n",
    "            model.pop[index];\n",
    "            \n",
    "            temp.build(a)\n",
    "            temp.set_mask(i.get_mask())\n",
    "            model.add(temp,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pruned dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
      "(None, 256)\n",
      "<class 'tuple'>\n",
      "10000/10000 [==============================] - 44s 4ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "#Discarded\n",
    "\n",
    "def convert_to_masked_model(model):\n",
    "    batch_norm_alpha=0.9\n",
    "    batch_norm_eps=1e-4\n",
    "\n",
    "    model=Sequential()\n",
    "    print(dir(model.layers))\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    print(model.output_shape)\n",
    "    print(type(model.output_shape))\n",
    "    temp = pruned_Dense(512)\n",
    "    haha = model.output_shape;\n",
    "    temp.build([haha[0],512])\n",
    "    model.add(temp)\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    temp = pruned_Dense(model.output_shape[1])\n",
    "    haha = model.output_shape;\n",
    "    temp.build([haha[0],512])\n",
    "    model.add(temp)\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    temp2 = pruned_Dense(10)\n",
    "    haha = model.output_shape;\n",
    "    temp2.build([haha[0],512])\n",
    "    model.add(temp2)\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "#convert the layers to maskable_layers:\n",
    "prunable_model=convert_to_masked_model(model)\n",
    "    \n",
    "weights_path='pretrained_cifar10.h5'\n",
    "prunable_model.load_weights(weights_path)\n",
    "\n",
    "for i in prunable_model.layers:\n",
    "    if type(i) == pruned_Dense:\n",
    "        i.set_mask(i.get_mask())\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "prunable_model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "scores_dropout_hidden = prunable_model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))\n",
    "\n",
    "#do the rest of the project:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify pruned_Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_inbound_node', '_get_node_attribute_at_index', '_node_key', 'add_loss', 'add_update', 'add_weight', 'assert_input_compatibility', 'build', 'built', 'call', 'compute_mask', 'compute_output_shape', 'count_params', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_losses_for', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_updates_for', 'get_weights', 'input', 'input_mask', 'input_shape', 'losses', 'non_trainable_weights', 'output', 'output_mask', 'output_shape', 'set_weights', 'trainable_weights', 'updates', 'weights']\n"
     ]
    }
   ],
   "source": [
    "print(dir(Dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_inbound_node', '_get_node_attribute_at_index', '_node_key', 'add_loss', 'add_update', 'add_weight', 'assert_input_compatibility', 'build', 'built', 'call', 'compute_mask', 'compute_output_shape', 'count_params', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_losses_for', 'get_mask', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_updates_for', 'get_weights', 'input', 'input_mask', 'input_shape', 'losses', 'non_trainable_weights', 'output', 'output_mask', 'output_shape', 'set_mask', 'set_weights', 'trainable_weights', 'updates', 'weights']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pruned_Dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'conv2d_319/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref>, <tf.Variable 'conv2d_319/bias:0' shape=(64,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(prunable_model.layers[0].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
      "Requirement already satisfied: pyyaml in c:\\python\\lib\\site-packages (from keras==2.2.0) (5.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\python\\lib\\site-packages (from keras==2.2.0) (1.16.2)\n",
      "Collecting keras-preprocessing==1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\python\\lib\\site-packages (from keras==2.2.0) (1.2.1)\n",
      "Requirement already satisfied: h5py in c:\\python\\lib\\site-packages (from keras==2.2.0) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\python\\lib\\site-packages (from keras==2.2.0) (1.12.0)\n",
      "Collecting keras-applications==1.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "  Found existing installation: Keras-Preprocessing 1.0.9\n",
      "    Uninstalling Keras-Preprocessing-1.0.9:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
      "  Found existing installation: Keras-Applications 1.0.7\n",
      "    Uninstalling Keras-Applications-1.0.7:\n",
      "      Successfully uninstalled Keras-Applications-1.0.7\n",
      "  Found existing installation: Keras 2.3.1\n",
      "    Uninstalling Keras-2.3.1:\n",
      "      Successfully uninstalled Keras-2.3.1\n",
      "Successfully installed keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow 1.12.0 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\n",
      "ERROR: tensorflow 1.12.0 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class _Conv(Layer):\n",
    "\n",
    "    def __init__(self, rank,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(_Conv, self).__init__(**kwargs)\n",
    "        self.rank = rank\n",
    "        self.filters = filters\n",
    "        self.kernel_size = normalize_tuple(kernel_size, rank,\n",
    "                                                      'kernel_size')\n",
    "        self.strides = normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = normalize_padding(padding)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.dilation_rate = normalize_tuple(dilation_rate, rank,\n",
    "                                                        'dilation_rate')\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.rank == 1:\n",
    "            outputs = K.conv1d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides[0],\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate[0])\n",
    "        if self.rank == 2:\n",
    "            outputs = K.conv2d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.rank == 3:\n",
    "            outputs = K.conv3d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            space = input_shape[1:-1]\n",
    "        elif self.data_format == 'channels_first':\n",
    "            space = input_shape[2:]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_output_length(\n",
    "                space[i],\n",
    "                self.kernel_size[i],\n",
    "                padding=self.padding,\n",
    "                stride=self.strides[i],\n",
    "                dilation=self.dilation_rate[i])\n",
    "            new_space.append(new_dim)\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n",
    "        elif self.data_format == 'channels_first':\n",
    "            return (input_shape[0], self.filters) + tuple(new_space)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'rank': self.rank,\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(_Conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pruned_Dense(Layer):\n",
    "    def __init__(self, n_neurons_out, **kwargs):\n",
    "        self.n_neurons_out = n_neurons_out\n",
    "        super(pruned_Dense,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #define the variables of this layer in the build function:\n",
    "        n_neurons_in = input_shape[1]\n",
    "        # print(n_neurons_in)\n",
    "        # print(self.n_neurons_out)\n",
    "        stdv = 1/np.sqrt(n_neurons_in)\n",
    "        w = np.random.normal(size=[n_neurons_in, self.n_neurons_out], loc=0.0, scale=stdv).astype(np.float32)\n",
    "        self.w = K.variable(w)\n",
    "        b = np.zeros(self.n_neurons_out)\n",
    "        self.b = K.variable(b)\n",
    "        # w is the weight matrix, b is the bias. These are the trainable variables of this layer.\n",
    "        self.trainable_weights = [self.w, self.b]\n",
    "        # mask is a non-trainable weight that simulates pruning. the values of mask should be either 1 or 0, where 0 will prune a weight. We initialize mask to all ones:\n",
    "        mask = np.ones((n_neurons_in, self.n_neurons_out))\n",
    "        self.mask = K.variable(mask)\n",
    "\n",
    "    def call(self, x):\n",
    "        # define the input-output relationship in this layer in this function\n",
    "        pruned_w = self.w * self.mask\n",
    "        out = K.dot(x, pruned_w)\n",
    "        out = out + self.b\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #define the shape of this layer's output:\n",
    "        return (input_shape[0], self.n_neurons_out)\n",
    "\n",
    "    def get_mask(self):\n",
    "        #get the mask values\n",
    "        return K.get_value(self.mask)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        #set new mask values to this layer\n",
    "        K.set_value(self.mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n",
    "           data_format=None, dilation_rate=(1, 1)):\n",
    "    \"\"\"2D convolution.\n",
    "    # Arguments\n",
    "        x: Tensor or variable.\n",
    "        kernel: kernel tensor.\n",
    "        strides: strides tuple.\n",
    "        padding: string, `\"same\"` or `\"valid\"`.\n",
    "        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
    "            Whether to use Theano or TensorFlow/CNTK data format\n",
    "            for inputs/kernels/outputs.\n",
    "        dilation_rate: tuple of 2 integers.\n",
    "    # Returns\n",
    "        A tensor, result of 2D convolution.\n",
    "    # Raises\n",
    "        ValueError: If `data_format` is neither\n",
    "            `\"channels_last\"` nor `\"channels_first\"`.\n",
    "    \"\"\"\n",
    "    data_format = normalize_data_format(data_format)\n",
    "\n",
    "    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
    "\n",
    "    padding = _preprocess_padding(padding)\n",
    "\n",
    "    # TF 2 arg conversion\n",
    "    kwargs = {}\n",
    "    if _is_tf_1():\n",
    "        kwargs['dilation_rate'] = dilation_rate\n",
    "    else:\n",
    "        kwargs['dilations'] = dilation_rate\n",
    "\n",
    "    x = tf.nn.convolution(\n",
    "        x, kernel,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        data_format=tf_data_format,\n",
    "        **kwargs)\n",
    "    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
    "        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n",
    "    \"\"\"Transpose and cast the input before the conv2d.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
    "        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n",
    "                        if the `data_format` is `\"channels_first\"`.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    # tensorflow doesn't support float64 for conv layer before 1.8.0\n",
    "    if (dtype(x) == 'float64' and\n",
    "            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n",
    "        x = tf.cast(x, 'float32')\n",
    "    tf_data_format = 'NHWC'\n",
    "    if data_format == 'channels_first':\n",
    "        if not _has_nchw_support() or force_transpose:\n",
    "            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n",
    "        else:\n",
    "            tf_data_format = 'NCHW'\n",
    "    return x, tf_data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__conv_utils__\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_tuple(value, n, name):\n",
    "    \"\"\"Transforms a single int or iterable of ints into an int tuple.\n",
    "    # Arguments\n",
    "        value: The value to validate and convert. Could be an int, or any iterable\n",
    "          of ints.\n",
    "        n: The size of the tuple to be returned.\n",
    "        name: The name of the argument being validated, e.g. `strides` or\n",
    "          `kernel_size`. This is only used to format error messages.\n",
    "    # Returns\n",
    "        A tuple of n integers.\n",
    "    # Raises\n",
    "        ValueError: If something else than an int/long or iterable thereof was\n",
    "        passed.\n",
    "    \"\"\"\n",
    "    if isinstance(value, int):\n",
    "        return (value,) * n\n",
    "    else:\n",
    "        try:\n",
    "            value_tuple = tuple(value)\n",
    "        except TypeError:\n",
    "            raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                             'integers. Received: {}'.format(name, n, value))\n",
    "        if len(value_tuple) != n:\n",
    "            raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                             'integers. Received: {}'.format(name, n, value))\n",
    "        for single_value in value_tuple:\n",
    "            try:\n",
    "                int(single_value)\n",
    "            except ValueError:\n",
    "                raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                                 'integers. Received: {} including element {} '\n",
    "                                 'of type {}'.format(name, n, value, single_value,\n",
    "                                                     type(single_value)))\n",
    "    return value_tuple\n",
    "\n",
    "\n",
    "def normalize_padding(value):\n",
    "    padding = value.lower()\n",
    "    allowed = {'valid', 'same', 'causal'}\n",
    "    if K.backend() == 'theano':\n",
    "        allowed.add('full')\n",
    "    if padding not in allowed:\n",
    "        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" '\n",
    "                         '(or \"causal\" for Conv1D). Received: {}'.format(padding))\n",
    "    return padding\n",
    "\n",
    "\n",
    "def convert_kernel(kernel):\n",
    "    \"\"\"Converts a Numpy kernel matrix from Theano format to TensorFlow format.\n",
    "    Also works reciprocally, since the transformation is its own inverse.\n",
    "    # Arguments\n",
    "        kernel: Numpy array (3D, 4D or 5D).\n",
    "    # Returns\n",
    "        The converted kernel.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid kernel shape or invalid data_format.\n",
    "    \"\"\"\n",
    "    kernel = np.asarray(kernel)\n",
    "    if not 3 <= kernel.ndim <= 5:\n",
    "        raise ValueError('Invalid kernel shape:', kernel.shape)\n",
    "    slices = [slice(None, None, -1) for _ in range(kernel.ndim)]\n",
    "    no_flip = (slice(None, None), slice(None, None))\n",
    "    slices[-2:] = no_flip\n",
    "    return np.copy(kernel[tuple(slices)])\n",
    "\n",
    "\n",
    "def conv_output_length(input_length, filter_size,\n",
    "                       padding, stride, dilation=1):\n",
    "    \"\"\"Determines output length of a convolution given input length.\n",
    "    # Arguments\n",
    "        input_length: integer.\n",
    "        filter_size: integer.\n",
    "        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        stride: integer.\n",
    "        dilation: dilation rate, integer.\n",
    "    # Returns\n",
    "        The output length (integer).\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert padding in {'same', 'valid', 'full', 'causal'}\n",
    "    dilated_filter_size = (filter_size - 1) * dilation + 1\n",
    "    if padding == 'same':\n",
    "        output_length = input_length\n",
    "    elif padding == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    elif padding == 'causal':\n",
    "        output_length = input_length\n",
    "    elif padding == 'full':\n",
    "        output_length = input_length + dilated_filter_size - 1\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "\n",
    "def conv_input_length(output_length, filter_size, padding, stride):\n",
    "    \"\"\"Determines input length of a convolution given output length.\n",
    "    # Arguments\n",
    "        output_length: integer.\n",
    "        filter_size: integer.\n",
    "        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        stride: integer.\n",
    "    # Returns\n",
    "        The input length (integer).\n",
    "    \"\"\"\n",
    "    if output_length is None:\n",
    "        return None\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    if padding == 'same':\n",
    "        pad = filter_size // 2\n",
    "    elif padding == 'valid':\n",
    "        pad = 0\n",
    "    elif padding == 'full':\n",
    "        pad = filter_size - 1\n",
    "    return (output_length - 1) * stride - 2 * pad + filter_size\n",
    "\n",
    "\n",
    "def deconv_length(dim_size, stride_size, kernel_size, padding,\n",
    "                  output_padding, dilation=1):\n",
    "    \"\"\"Determines output length of a transposed convolution given input length.\n",
    "    # Arguments\n",
    "        dim_size: Integer, the input length.\n",
    "        stride_size: Integer, the stride along the dimension of `dim_size`.\n",
    "        kernel_size: Integer, the kernel size along the dimension of\n",
    "            `dim_size`.\n",
    "        padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        output_padding: Integer, amount of padding along the output dimension,\n",
    "            Can be set to `None` in which case the output length is inferred.\n",
    "        dilation: dilation rate, integer.\n",
    "    # Returns\n",
    "        The output length (integer).\n",
    "    \"\"\"\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    if dim_size is None:\n",
    "        return None\n",
    "\n",
    "    # Get the dilated kernel size\n",
    "    kernel_size = (kernel_size - 1) * dilation + 1\n",
    "\n",
    "    # Infer length if output padding is None, else compute the exact length\n",
    "    if output_padding is None:\n",
    "        if padding == 'valid':\n",
    "            dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\n",
    "        elif padding == 'full':\n",
    "            dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\n",
    "        elif padding == 'same':\n",
    "            dim_size = dim_size * stride_size\n",
    "    else:\n",
    "        if padding == 'same':\n",
    "            pad = kernel_size // 2\n",
    "        elif padding == 'valid':\n",
    "            pad = 0\n",
    "        elif padding == 'full':\n",
    "            pad = kernel_size - 1\n",
    "\n",
    "        dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad +\n",
    "                    output_padding)\n",
    "\n",
    "    return dim_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pruned_Conv2D(_Conv): \n",
    "\n",
    "    #@interfaces.legacy_conv2d_support\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(pruned_Conv2D, self).__init__(\n",
    "            rank=2,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Conv2D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        return config\n",
    "    \n",
    "    #def build(self, input_shape):\n",
    "    #    return\n",
    "    def call(self, x):\n",
    "        return\n",
    "    #def get_output_shape_for(self,input_shape):\n",
    "    #    return\n",
    "    def get_mask(self):\n",
    "        return self.kernel\n",
    "    def set_mask(self, mask):\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-354454047c34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpruned_Conv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_norm_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_norm_eps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-cb36a650a203>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3317c7b6bf59>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m                  \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                  **kwargs):\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "\n",
    "batch_norm_alpha=0.9\n",
    "batch_norm_eps=1e-4\n",
    "\n",
    "model=Sequential()\n",
    "print(dir(model.layers))\n",
    "model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "print(type(model.output_shape))\n",
    "temp = pruned_Dense(512)\n",
    "haha = model.output_shape;\n",
    "temp.build([haha[0],512])\n",
    "model.add(temp)\n",
    "model.add(Activation('relu'))\n",
    "    \n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "temp = pruned_Dense(model.output_shape[1])\n",
    "haha = model.output_shape;\n",
    "temp.build([haha[0],512])\n",
    "model.add(temp)\n",
    "model.add(Activation('relu'))\n",
    "    \n",
    "model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "temp2 = pruned_Dense(10)\n",
    "haha = model.output_shape;\n",
    "temp2.build([haha[0],512])\n",
    "model.add(temp2)\n",
    "model.add(Activation('softmax'))\n",
    "    \n",
    "#convert the layers to maskable_layers:\n",
    "prunable_model= model\n",
    "    \n",
    "weights_path='pretrained_cifar10.h5'\n",
    "prunable_model.load_weights(weights_path)\n",
    "\n",
    "for i in prunable_model.layers:\n",
    "    if type(i) == pruned_Dense:\n",
    "        i.set_mask(i.get_mask())\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "prunable_model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "scores_dropout_hidden = prunable_model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))\n",
    "\n",
    "#do the rest of the project:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class pruned_Dense(Layer):\n",
    "    def __init__(self, n_neurons_out, **kwargs):\n",
    "        self.n_neurons_out = n_neurons_out\n",
    "        super(pruned_Dense,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #define the variables of this layer in the build function:\n",
    "        n_neurons_in = input_shape[1]\n",
    "        # print(n_neurons_in)\n",
    "        # print(self.n_neurons_out)\n",
    "        stdv = 1/np.sqrt(n_neurons_in)\n",
    "        w = np.random.normal(size=[n_neurons_in, self.n_neurons_out], loc=0.0, scale=stdv).astype(np.float32)\n",
    "        self.w = K.variable(w)\n",
    "        b = np.zeros(self.n_neurons_out)\n",
    "        self.b = K.variable(b)\n",
    "        # w is the weight matrix, b is the bias. These are the trainable variables of this layer.\n",
    "        self.trainable_weights = [self.w, self.b]\n",
    "        # mask is a non-trainable weight that simulates pruning. the values of mask should be either 1 or 0, where 0 will prune a weight. We initialize mask to all ones:\n",
    "        mask = np.ones((n_neurons_in, self.n_neurons_out))\n",
    "        self.mask = K.variable(mask)\n",
    "\n",
    "    def call(self, x):\n",
    "        # define the input-output relationship in this layer in this function\n",
    "        pruned_w = self.w * self.mask\n",
    "        out = K.dot(x, pruned_w)\n",
    "        out = out + self.b\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #define the shape of this layer's output:\n",
    "        return (input_shape[0], self.n_neurons_out)\n",
    "\n",
    "    def get_mask(self):\n",
    "        #get the mask values\n",
    "        return K.get_value(self.mask)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        #set new mask values to this layer\n",
    "        K.set_value(self.mask, mask)\n",
    "\n",
    "class pruned_Conv2D(Layer):\n",
    "    def __init__(self,n_neurons_out):\n",
    "        self.n_neurons_out = n_neurons_out\n",
    "        super(pruned_Conv2D,self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "    def call(self, x):\n",
    "        return\n",
    "    def get_output_shape_for(self,input_shape):\n",
    "        return\n",
    "    def get_mask(self):\n",
    "        return\n",
    "    def set_mask(self, mask):\n",
    "        return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
