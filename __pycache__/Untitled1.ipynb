{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#__conv_utils__\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "\n",
    "def normalize_tuple(value, n, name):\n",
    "    \"\"\"Transforms a single int or iterable of ints into an int tuple.\n",
    "    # Arguments\n",
    "        value: The value to validate and convert. Could be an int, or any iterable\n",
    "          of ints.\n",
    "        n: The size of the tuple to be returned.\n",
    "        name: The name of the argument being validated, e.g. `strides` or\n",
    "          `kernel_size`. This is only used to format error messages.\n",
    "    # Returns\n",
    "        A tuple of n integers.\n",
    "    # Raises\n",
    "        ValueError: If something else than an int/long or iterable thereof was\n",
    "        passed.\n",
    "    \"\"\"\n",
    "    if isinstance(value, int):\n",
    "        return (value,) * n\n",
    "    else:\n",
    "        try:\n",
    "            value_tuple = tuple(value)\n",
    "        except TypeError:\n",
    "            raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                             'integers. Received: {}'.format(name, n, value))\n",
    "        if len(value_tuple) != n:\n",
    "            raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                             'integers. Received: {}'.format(name, n, value))\n",
    "        for single_value in value_tuple:\n",
    "            try:\n",
    "                int(single_value)\n",
    "            except ValueError:\n",
    "                raise ValueError('The `{}` argument must be a tuple of {} '\n",
    "                                 'integers. Received: {} including element {} '\n",
    "                                 'of type {}'.format(name, n, value, single_value,\n",
    "                                                     type(single_value)))\n",
    "    return value_tuple\n",
    "\n",
    "\n",
    "def normalize_padding(value):\n",
    "    padding = value.lower()\n",
    "    allowed = {'valid', 'same', 'causal'}\n",
    "    if K.backend() == 'theano':\n",
    "        allowed.add('full')\n",
    "    if padding not in allowed:\n",
    "        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" '\n",
    "                         '(or \"causal\" for Conv1D). Received: {}'.format(padding))\n",
    "    return padding\n",
    "\n",
    "\n",
    "def convert_kernel(kernel):\n",
    "    \"\"\"Converts a Numpy kernel matrix from Theano format to TensorFlow format.\n",
    "    Also works reciprocally, since the transformation is its own inverse.\n",
    "    # Arguments\n",
    "        kernel: Numpy array (3D, 4D or 5D).\n",
    "    # Returns\n",
    "        The converted kernel.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid kernel shape or invalid data_format.\n",
    "    \"\"\"\n",
    "    kernel = np.asarray(kernel)\n",
    "    if not 3 <= kernel.ndim <= 5:\n",
    "        raise ValueError('Invalid kernel shape:', kernel.shape)\n",
    "    slices = [slice(None, None, -1) for _ in range(kernel.ndim)]\n",
    "    no_flip = (slice(None, None), slice(None, None))\n",
    "    slices[-2:] = no_flip\n",
    "    return np.copy(kernel[tuple(slices)])\n",
    "\n",
    "\n",
    "def conv_output_length(input_length, filter_size,\n",
    "                       padding, stride, dilation=1):\n",
    "    \"\"\"Determines output length of a convolution given input length.\n",
    "    # Arguments\n",
    "        input_length: integer.\n",
    "        filter_size: integer.\n",
    "        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        stride: integer.\n",
    "        dilation: dilation rate, integer.\n",
    "    # Returns\n",
    "        The output length (integer).\n",
    "    \"\"\"\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    assert padding in {'same', 'valid', 'full', 'causal'}\n",
    "    dilated_filter_size = (filter_size - 1) * dilation + 1\n",
    "    if padding == 'same':\n",
    "        output_length = input_length\n",
    "    elif padding == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    elif padding == 'causal':\n",
    "        output_length = input_length\n",
    "    elif padding == 'full':\n",
    "        output_length = input_length + dilated_filter_size - 1\n",
    "    return (output_length + stride - 1) // stride\n",
    "\n",
    "\n",
    "def conv_input_length(output_length, filter_size, padding, stride):\n",
    "    \"\"\"Determines input length of a convolution given output length.\n",
    "    # Arguments\n",
    "        output_length: integer.\n",
    "        filter_size: integer.\n",
    "        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        stride: integer.\n",
    "    # Returns\n",
    "        The input length (integer).\n",
    "    \"\"\"\n",
    "    if output_length is None:\n",
    "        return None\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    if padding == 'same':\n",
    "        pad = filter_size // 2\n",
    "    elif padding == 'valid':\n",
    "        pad = 0\n",
    "    elif padding == 'full':\n",
    "        pad = filter_size - 1\n",
    "    return (output_length - 1) * stride - 2 * pad + filter_size\n",
    "\n",
    "\n",
    "def deconv_length(dim_size, stride_size, kernel_size, padding,\n",
    "                  output_padding, dilation=1):\n",
    "    \"\"\"Determines output length of a transposed convolution given input length.\n",
    "    # Arguments\n",
    "        dim_size: Integer, the input length.\n",
    "        stride_size: Integer, the stride along the dimension of `dim_size`.\n",
    "        kernel_size: Integer, the kernel size along the dimension of\n",
    "            `dim_size`.\n",
    "        padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
    "        output_padding: Integer, amount of padding along the output dimension,\n",
    "            Can be set to `None` in which case the output length is inferred.\n",
    "        dilation: dilation rate, integer.\n",
    "    # Returns\n",
    "        The output length (integer).\n",
    "    \"\"\"\n",
    "    assert padding in {'same', 'valid', 'full'}\n",
    "    if dim_size is None:\n",
    "        return None\n",
    "\n",
    "    # Get the dilated kernel size\n",
    "    kernel_size = (kernel_size - 1) * dilation + 1\n",
    "\n",
    "    # Infer length if output padding is None, else compute the exact length\n",
    "    if output_padding is None:\n",
    "        if padding == 'valid':\n",
    "            dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\n",
    "        elif padding == 'full':\n",
    "            dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\n",
    "        elif padding == 'same':\n",
    "            dim_size = dim_size * stride_size\n",
    "    else:\n",
    "        if padding == 'same':\n",
    "            pad = kernel_size // 2\n",
    "        elif padding == 'valid':\n",
    "            pad = 0\n",
    "        elif padding == 'full':\n",
    "            pad = kernel_size - 1\n",
    "\n",
    "        dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad +\n",
    "                    output_padding)\n",
    "\n",
    "    return dim_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n",
    "    \"\"\"Transpose and cast the input before the conv2d.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
    "        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n",
    "                        if the `data_format` is `\"channels_first\"`.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    # tensorflow doesn't support float64 for conv layer before 1.8.0\n",
    "    if (dtype(x) == 'float64' and\n",
    "            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):\n",
    "        x = tf.cast(x, 'float32')\n",
    "    tf_data_format = 'NHWC'\n",
    "    if data_format == 'channels_first':\n",
    "        if not _has_nchw_support() or force_transpose:\n",
    "            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n",
    "        else:\n",
    "            tf_data_format = 'NCHW'\n",
    "    return x, tf_data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n",
    "           data_format=None, dilation_rate=(1, 1)):\n",
    "    \"\"\"2D convolution.\n",
    "    # Arguments\n",
    "        x: Tensor or variable.\n",
    "        kernel: kernel tensor.\n",
    "        strides: strides tuple.\n",
    "        padding: string, `\"same\"` or `\"valid\"`.\n",
    "        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
    "            Whether to use Theano or TensorFlow/CNTK data format\n",
    "            for inputs/kernels/outputs.\n",
    "        dilation_rate: tuple of 2 integers.\n",
    "    # Returns\n",
    "        A tensor, result of 2D convolution.\n",
    "    # Raises\n",
    "        ValueError: If `data_format` is neither\n",
    "            `\"channels_last\"` nor `\"channels_first\"`.\n",
    "    \"\"\"\n",
    "    data_format = normalize_data_format(data_format)\n",
    "\n",
    "    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
    "\n",
    "    padding = _preprocess_padding(padding)\n",
    "\n",
    "    # TF 2 arg conversion\n",
    "    kwargs = {}\n",
    "    if _is_tf_1():\n",
    "        kwargs['dilation_rate'] = dilation_rate\n",
    "    else:\n",
    "        kwargs['dilations'] = dilation_rate\n",
    "\n",
    "    x = tf.nn.convolution(\n",
    "        x, kernel,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        data_format=tf_data_format,\n",
    "        **kwargs)\n",
    "    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
    "        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pruned_Dense(Layer):\n",
    "    def __init__(self, n_neurons_out, **kwargs):\n",
    "        self.n_neurons_out = n_neurons_out\n",
    "        super(pruned_Dense,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #define the variables of this layer in the build function:\n",
    "        n_neurons_in = input_shape[1]\n",
    "        # print(n_neurons_in)\n",
    "        # print(self.n_neurons_out)\n",
    "        stdv = 1/np.sqrt(n_neurons_in)\n",
    "        w = np.random.normal(size=[n_neurons_in, self.n_neurons_out], loc=0.0, scale=stdv).astype(np.float32)\n",
    "        self.w = K.variable(w)\n",
    "        b = np.zeros(self.n_neurons_out)\n",
    "        self.b = K.variable(b)\n",
    "        # w is the weight matrix, b is the bias. These are the trainable variables of this layer.\n",
    "        self.trainable_weights = [self.w, self.b]\n",
    "        # mask is a non-trainable weight that simulates pruning. the values of mask should be either 1 or 0, where 0 will prune a weight. We initialize mask to all ones:\n",
    "        mask = np.ones((n_neurons_in, self.n_neurons_out))\n",
    "        self.mask = K.variable(mask)\n",
    "        print(\"initialized a dense\")\n",
    "\n",
    "    def call(self, x):\n",
    "        # define the input-output relationship in this layer in this function\n",
    "        pruned_w = self.w * self.mask\n",
    "        out = K.dot(x, pruned_w)\n",
    "        out = out + self.b\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #define the shape of this layer's output:\n",
    "        return (input_shape[0], self.n_neurons_out)\n",
    "\n",
    "    def get_mask(self):\n",
    "        #get the mask values\n",
    "        return K.get_value(self.mask)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        #set new mask values to this layer\n",
    "        K.set_value(self.mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_format(value):\n",
    "    \"\"\"Checks that the value correspond to a valid data format.\n",
    "    # Arguments\n",
    "        value: String or None. `'channels_first'` or `'channels_last'`.\n",
    "    # Returns\n",
    "        A string, either `'channels_first'` or `'channels_last'`\n",
    "    # Example\n",
    "    ```python\n",
    "        >>> from keras import backend as K\n",
    "        >>> K.normalize_data_format(None)\n",
    "        'channels_first'\n",
    "        >>> K.normalize_data_format('channels_last')\n",
    "        'channels_last'\n",
    "    ```\n",
    "    # Raises\n",
    "        ValueError: if `value` or the global `data_format` invalid.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        value = image_data_format()\n",
    "    data_format = value.lower()\n",
    "    if data_format not in {'channels_first', 'channels_last'}:\n",
    "        raise ValueError('The `data_format` argument must be one of '\n",
    "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
    "                         str(value))\n",
    "    return data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSpec(object):\n",
    "    \"\"\"Specifies the ndim, dtype and shape of every input to a layer.\n",
    "    Every layer should expose (if appropriate) an `input_spec` attribute:\n",
    "    a list of instances of InputSpec (one per input tensor).\n",
    "    A None entry in a shape is compatible with any dimension,\n",
    "    a None shape is compatible with any shape.\n",
    "    # Arguments\n",
    "        dtype: Expected datatype of the input.\n",
    "        shape: Shape tuple, expected shape of the input\n",
    "            (may include None for unchecked axes).\n",
    "        ndim: Integer, expected rank of the input.\n",
    "        max_ndim: Integer, maximum rank of the input.\n",
    "        min_ndim: Integer, minimum rank of the input.\n",
    "        axes: Dictionary mapping integer axes to\n",
    "            a specific dimension value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dtype=None,\n",
    "                 shape=None,\n",
    "                 ndim=None,\n",
    "                 max_ndim=None,\n",
    "                 min_ndim=None,\n",
    "                 axes=None):\n",
    "        self.dtype = dtype\n",
    "        self.shape = shape\n",
    "        if shape is not None:\n",
    "            self.ndim = len(shape)\n",
    "        else:\n",
    "            self.ndim = ndim\n",
    "        self.max_ndim = max_ndim\n",
    "        self.min_ndim = min_ndim\n",
    "        self.axes = axes or {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        spec = [('dtype=' + str(self.dtype)) if self.dtype else '',\n",
    "                ('shape=' + str(self.shape)) if self.shape else '',\n",
    "                ('ndim=' + str(self.ndim)) if self.ndim else '',\n",
    "                ('max_ndim=' + str(self.max_ndim)) if self.max_ndim else '',\n",
    "                ('min_ndim=' + str(self.min_ndim)) if self.min_ndim else '',\n",
    "                ('axes=' + str(self.axes)) if self.axes else '']\n",
    "        return 'InputSpec(%s)' % ', '.join(x for x in spec if x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import activations\n",
    "import initializers\n",
    "import regularizers\n",
    "import constraints\n",
    "class _Conv2(Layer):\n",
    "\n",
    "    def __init__(self, rank,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(_Conv2, self).__init__(**kwargs)\n",
    "        self.rank = rank\n",
    "        self.filters = filters\n",
    "        self.kernel_size = normalize_tuple(kernel_size, rank,\n",
    "                                                      'kernel_size')\n",
    "        self.strides = normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = normalize_padding(padding)\n",
    "        self.data_format = normalize_data_format(data_format)\n",
    "        self.dilation_rate = normalize_tuple(dilation_rate, rank,\n",
    "                                                        'dilation_rate')\n",
    "        self.activation =activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.rank == 1:\n",
    "            outputs = K.conv1d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides[0],\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate[0])\n",
    "        if self.rank == 2:\n",
    "            outputs = K.conv2d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        if self.rank == 3:\n",
    "            outputs = K.conv3d(\n",
    "                inputs,\n",
    "                self.kernel,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            space = input_shape[1:-1]\n",
    "        elif self.data_format == 'channels_first':\n",
    "            space = input_shape[2:]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_output_length(\n",
    "                space[i],\n",
    "                self.kernel_size[i],\n",
    "                padding=self.padding,\n",
    "                stride=self.strides[i],\n",
    "                dilation=self.dilation_rate[i])\n",
    "            new_space.append(new_dim)\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n",
    "        elif self.data_format == 'channels_first':\n",
    "            return (input_shape[0], self.filters) + tuple(new_space)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'rank': self.rank,\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(_Conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pruned_Conv2D(_Conv2): \n",
    "\n",
    "    #@interfaces.legacy_conv2d_support\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(pruned_Conv2D, self).__init__(\n",
    "            rank=2,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Conv2D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        return config\n",
    "    \n",
    "    #def build(self, input_shape):\n",
    "    #    return\n",
    "    #def call(self, x):\n",
    "    #    return\n",
    "    #def get_output_shape_for(self,input_shape):\n",
    "    #    return\n",
    "    def get_mask(self):\n",
    "        return self.kernel\n",
    "    def set_mask(self, mask):\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_format():\n",
    "    \"\"\"Returns the default image data format convention.\n",
    "    # Returns\n",
    "        A string, either `'channels_first'` or `'channels_last'`\n",
    "    # Example\n",
    "    ```python\n",
    "        >>> keras.backend.image_data_format()\n",
    "        'channels_first'\n",
    "    ```\n",
    "    \"\"\"\n",
    "    _IMAGE_DATA_FORMAT = 'channels_last'\n",
    "    return _IMAGE_DATA_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "data_train = X_train\n",
    "labels_train = Y_train\n",
    "data_test = X_test\n",
    "labels_test = Y_test\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size=100\n",
    "lr=0.001\n",
    "Training=True\n",
    "Compressing=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    batch_norm_alpha=0.9\n",
    "    batch_norm_eps=1e-4\n",
    "\n",
    "    model=Sequential()\n",
    "    print(dir(model.layers))\n",
    "    model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    temp = pruned_Dense(512)\n",
    "    temp.build([None,512])\n",
    "    model.add(temp)\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    \n",
    "    temp = pruned_Dense(512)\n",
    "    temp.build([None,512])\n",
    "    model.add(temp)\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    \n",
    "    temp2 = pruned_Dense(10)\n",
    "    temp2.build([None,512])\n",
    "    model.add(temp2)\n",
    "    model.add(Activation('softmax'))\n",
    "    for i in model.layers:\n",
    "        print(i)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C089B38>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C089668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C0D5080>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C0D5C50>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C0E3EB8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C0E3D68>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E3C122F60>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C1D3F28>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C2A24E0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C242390>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C242CC0>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C3179E8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C317C88>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E3C35EDA0>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3CC4DF28>\n",
      "<keras.layers.core.Activation object at 0x0000020E40C89EB8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3CCB7C50>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3CCB7AC8>\n",
      "<keras.layers.core.Activation object at 0x0000020E4116EF28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4116EBA8>\n",
      "<keras.layers.core.Flatten object at 0x0000020E411ACF98>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DA02CF8>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DA02E48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4DAF8A90>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DAF88D0>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DB23748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4DB69CF8>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DB69CC0>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DC506D8>\n",
      "10000/10000 [==============================] - 37s 4ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "# 程序开始时声明\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# 在model加载前添加set_session\n",
    "set_session(sess)\n",
    "\n",
    "#convert the layers to maskable_layers:\n",
    "model = get_model()\n",
    "    \n",
    "weights_path='pretrained_cifar10.h5'\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "\n",
    "scores_dropout_hidden = model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.pruned_Conv2D object at 0x0000020E3C089B38>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C089668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C0D5080>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C0D5C50>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C0E3EB8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C0E3D68>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E3C122F60>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C1D3F28>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C2A24E0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C242390>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3C242CC0>\n",
      "<keras.layers.core.Activation object at 0x0000020E3C3179E8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3C317C88>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E3C35EDA0>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3CC4DF28>\n",
      "<keras.layers.core.Activation object at 0x0000020E40C89EB8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E3CCB7C50>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E3CCB7AC8>\n",
      "<keras.layers.core.Activation object at 0x0000020E4116EF28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4116EBA8>\n",
      "<keras.layers.core.Flatten object at 0x0000020E411ACF98>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DA02CF8>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DA02E48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4DAF8A90>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DAF88D0>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DB23748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E4DB69CF8>\n",
      "<__main__.pruned_Dense object at 0x0000020E4DB69CC0>\n",
      "<keras.layers.core.Activation object at 0x0000020E4DC506D8>\n",
      "10000/10000 [==============================] - 36s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "global sess\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "for i in model.layers:\n",
    "    print(i)\n",
    "    if type(i) == pruned_Dense:\n",
    "        #print(dir(i))\n",
    "        #print(K.get_value(i.mask))\n",
    "        i.set_mask(i.get_mask())\n",
    "scores_dropout_hidden = model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking conv2d content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned__conv2d_19\n",
      "(3, 3, 3, 64)\n",
      "pruned__conv2d_20\n",
      "(3, 3, 64, 64)\n",
      "pruned__conv2d_21\n",
      "(3, 3, 64, 128)\n",
      "pruned__conv2d_22\n",
      "(3, 3, 128, 128)\n",
      "pruned__conv2d_23\n",
      "(3, 3, 128, 256)\n",
      "pruned__conv2d_24\n",
      "(3, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "global sess\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "for i in model.layers:\n",
    "    if type(i) == pruned_Conv2D:\n",
    "        print(i.name)\n",
    "        #print(K.get_value(i.mask))\n",
    "        print(i.get_mask())\n",
    "        #print(dir(i.get_mask()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    batch_norm_alpha=0.9\n",
    "    batch_norm_eps=1e-4\n",
    "\n",
    "    model=Sequential()\n",
    "    print(dir(model.layers))\n",
    "    model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    model.add(pruned_Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    temp = pruned_Dense(512)\n",
    "    temp.build([None,512])\n",
    "    model.add(temp)\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    \n",
    "    temp = pruned_Dense(512)\n",
    "    temp.build([None,512])\n",
    "    model.add(temp)\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "    \n",
    "    temp2 = pruned_Dense(10)\n",
    "    temp2.build([None,512])\n",
    "    model.add(temp2)\n",
    "    model.add(Activation('softmax'))\n",
    "    for i in model.layers:\n",
    "        print(i)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pruned_Conv2D(_Conv2): \n",
    "\n",
    "    #@interfaces.legacy_conv2d_support\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(pruned_Conv2D, self).__init__(\n",
    "            rank=2,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Conv2D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        return config\n",
    "    \n",
    "    #def build(self, input_shape):\n",
    "    #    return\n",
    "    #def call(self, x):\n",
    "    #    return\n",
    "    #def get_output_shape_for(self,input_shape):\n",
    "    #    return\n",
    "    def get_mask(self):\n",
    "        a = self.kernel.get_shape();\n",
    "        masker = np.random.rand(a.dims[0],a.dims[1],a.dims[2],a.dims[3]);\n",
    "        masker[masker[:,:,:,:]<0.25] = 0;\n",
    "        masker[masker[:,:,:,:]>=0.25] = 1;\n",
    "        valuebefore = K.get_value(self.kernel)\n",
    "        #print(masker)\n",
    "        #print(K.get_value(self.kernel))\n",
    "        masker = K.variable(masker)\n",
    "        self.kernel = self.kernel * masker\n",
    "        #print(K.get_value(self.kernel))\n",
    "        if(valuebefore.all() == K.get_value(self.kernel).all()):\n",
    "            print('no modification')\n",
    "        return self.kernel\n",
    "    def set_mask(self):\n",
    "        a = self.bias.get_shape();\n",
    "        #print(a)\n",
    "        masker = np.random.rand(a.dims[0]);\n",
    "\n",
    "        masker[masker[:]<0.25] = 0;\n",
    "        masker[masker[:]>=0.25] = 1;\n",
    "        #print(masker)\n",
    "        valuebefore = K.get_value(self.bias)\n",
    "        #print(K.get_value(self.bias))\n",
    "        masker = K.variable(masker)\n",
    "        self.bias = self.bias * masker#self.set_weights(self.bias * masker)\n",
    "        #print(K.get_value(self.bias))\n",
    "        #if(valuebefore.all() == K.get_value(self.bias).all()):\n",
    "        #    print('no modification')\n",
    "        return self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "<__main__.pruned_Conv2D object at 0x0000020E611E1CC0>\n",
      "<keras.layers.core.Activation object at 0x0000020E611E1F60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E611E1E10>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E611E1B38>\n",
      "<keras.layers.core.Activation object at 0x0000020E13E85C18>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E13E41E10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E13EBDB38>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E13F38978>\n",
      "<keras.layers.core.Activation object at 0x0000020E14004AC8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E13FA7128>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E13FA7080>\n",
      "<keras.layers.core.Activation object at 0x0000020E1407B9B0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E1407B710>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x0000020E141CFCC0>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E14280898>\n",
      "<keras.layers.core.Activation object at 0x0000020E1434BC50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E142EF2E8>\n",
      "<__main__.pruned_Conv2D object at 0x0000020E142EFB38>\n",
      "<keras.layers.core.Activation object at 0x0000020E1475A128>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E14713EF0>\n",
      "<keras.layers.core.Flatten object at 0x0000020E14799978>\n",
      "<__main__.pruned_Dense object at 0x0000020E14A3AE10>\n",
      "<keras.layers.core.Activation object at 0x0000020E14A3A6A0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E15054860>\n",
      "<__main__.pruned_Dense object at 0x0000020E15054550>\n",
      "<keras.layers.core.Activation object at 0x0000020E15086978>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000020E15143F98>\n",
      "<__main__.pruned_Dense object at 0x0000020E150BAA90>\n",
      "<keras.layers.core.Activation object at 0x0000020E151B5EF0>\n",
      "10000/10000 [==============================] - 61s 6ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "# 程序开始时声明\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# 在model加载前添加set_session\n",
    "set_session(sess)\n",
    "\n",
    "#convert the layers to maskable_layers:\n",
    "model = get_model()\n",
    "    \n",
    "weights_path='pretrained_cifar10.h5'\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "for i in model.layers:\n",
    "    if type(i) == pruned_Conv2D:\n",
    "        i.get_mask()\n",
    "        i.set_mask()\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "\n",
    "scores_dropout_hidden = model.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer changer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.pooling.MaxPooling2D'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.convolutional.Conv2D'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Flatten'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "<class 'keras.layers.normalization.BatchNormalization'>\n",
      "<class 'keras.layers.core.Dense'>\n",
      "<class 'keras.layers.core.Activation'>\n",
      "10000/10000 [==============================] - 46s 5ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, Lambda, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train=X_train.astype(np.float32)\n",
    "X_test=X_test.astype(np.float32)\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train=2*X_train-1\n",
    "X_test=2*X_test-1\n",
    "\n",
    "data_train = X_train\n",
    "labels_train = Y_train\n",
    "data_test = X_test\n",
    "labels_test = Y_test\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size=100\n",
    "lr=0.001\n",
    "Training=True\n",
    "Compressing=False\n",
    "\n",
    "def get_model():\n",
    "\tbatch_norm_alpha=0.9\n",
    "\tbatch_norm_eps=1e-4\n",
    "\n",
    "\tmodel=Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid',input_shape=[32,32,3]))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='valid'))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\t#model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\n",
    "\tmodel.add(Dense(512))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Dense(512))\n",
    "\tmodel.add(Activation('relu'))\n",
    "\tmodel.add(BatchNormalization(axis=-1, momentum=batch_norm_alpha, epsilon=batch_norm_eps))\n",
    "\tmodel.add(Dense(10))\n",
    "\tmodel.add(Activation('softmax'))\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "test=get_model()\n",
    "weights_path='pretrained_cifar10.h5'\n",
    "test.load_weights(weights_path)\n",
    "for i in test.layers:\n",
    "    print(type(i))\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "test.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "\n",
    "scores_dropout_hidden = test.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "initialized a dense\n",
      "10000/10000 [==============================] - 54s 5ms/step\n",
      "Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "# 程序开始时声明\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# 在model加载前添加set_session\n",
    "set_session(sess)\n",
    "\n",
    "def convert_to_masked_model(model):\n",
    "    batch_norm_alpha=0.9\n",
    "    batch_norm_eps=1e-4\n",
    "\n",
    "    model2=Sequential()\n",
    "    for i in model.layers:\n",
    "        #if type(i) == keras.layers.convolutional.Conv2D:\n",
    "        #if type(i) == keras.layers.core.Dense:\n",
    "        if type(i) != keras.layers.convolutional.Conv2D and type(i) != keras.layers.core.Dense:\n",
    "            model2.add(i)\n",
    "        elif type(i) == keras.layers.convolutional.Conv2D:\n",
    "            temp2 = pruned_Conv2D(filters=i.filters, kernel_size=3, strides=i.strides, padding='valid',input_shape=(i.input_shape[1],i.input_shape[2],i.input_shape[3]))\n",
    "            temp2.kernel = i.kernel\n",
    "            temp2.bias = i.bias\n",
    "            #temp2.set_weights()\n",
    "            model2.add(temp2)\n",
    "        else:\n",
    "            temp2 = pruned_Dense(i.output_shape[1])\n",
    "            temp3 = [1,int(i.input_shape[1])]\n",
    "            temp2.build(temp3)\n",
    "            temp2.set_weights(i.get_weights())\n",
    "            model2.add(temp2)\n",
    "    \n",
    "    return model2\n",
    "\n",
    "cask = convert_to_masked_model(test)\n",
    "opt = keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "#now complie the prunable model with sparse categorical crossentropy loss function as you did in part 1\n",
    "#make sure weights are loaded correctly by evaluating the prunable model here and printing the output\n",
    "cask.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\" ,metrics=['accuracy'])\n",
    "#history_dropout_hidden = model.fit(data_train, labels_train, validation_data=(data_test, labels_test), epochs=50, batch_size=1000, shuffle=True)\n",
    "\n",
    "scores_dropout_hidden = cask.evaluate(data_test, labels_test)\n",
    "print(\"Accuracy: %.2f%%\" %(scores_dropout_hidden[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
